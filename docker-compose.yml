services:
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    ports:
      - "3000:8080"
    environment:
      - WEBUI_ENV=production
      - NVIDIA_VISIBLE_DEVICES=all  # Use all available GPUs
    volumes:
      - openwebui_data:/app/backend/data  # Volume for OpenWebUI data (SQLite database and logs)
    depends_on:
      - backend
    restart: unless-stopped
    runtime: nvidia  # Ensure GPU access (if needed for specific containers)

  backend:
    build:
      context: .  # The context is the root of your project
      dockerfile: Dockerfile  # Path to the Dockerfile
    container_name: openwebui-rag-backend
    ports:
      - "5001:5001"
    volumes:
      - ./backend:/app/backend  # Mount local backend code
      - /home/ubuntu/openwebui-container/pptx_files:/app/pptx_files  # Mount volume for PowerPoint files
      - openwebui_data:/app/backend/data  # Mount volume for SQLite database and logs
      - ollama_data:/root/.ollama  # Mount volume for Ollama models
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - NVIDIA_VISIBLE_DEVICES=all  # Use all available GPUs
    env_file:
      - ./backend/.env
    depends_on:
      - ollama
    restart: unless-stopped
    runtime: nvidia  # Enable GPU access for backend (if required)

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama  # Volume for Ollama models
    restart: unless-stopped
    runtime: nvidia  # Enable GPU access for Ollama container
    environment:
      - NVIDIA_VISIBLE_DEVICES=all  # Use all available GPUs

volumes:
  openwebui_data:  # Volume for OpenWebUI data (SQLite database, logs, etc.)
  ollama_data:     # Volume for Ollama models
  pptx_files:      # Volume for PowerPoint files