services:
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui
    ports:
      - "3000:8080"
    environment:
      - WEBUI_ENV=production
    volumes:
      - openwebui_data:/app/backend/data  # Volume for OpenWebUI data (SQLite database and logs)
    depends_on:
      - backend
    restart: unless-stopped
    runtime: nvidia  # Ensure GPU access (if needed for specific containers)
    deploy:
      resources:
        limits:
          nvidia.com/gpu.count: 2

  backend:
    build:
      context: .  # The context is the root of your project
      dockerfile: Dockerfile  # Path to the Dockerfile
    container_name: openwebui-rag-backend
    ports:
      - "5001:5001"
    volumes:
      - ./backend:/app/backend  # Mount local backend code
      - pptx_files:/app/pptx_files  # Mount volume for PowerPoint files
      - openwebui_data:/app/backend/data  # Mount volume for SQLite database and logs
      - ollama_data:/root/.ollama  # Mount volume for Ollama models
    environment:
      - OLLAMA_HOST=http://ollama:11434
    env_file:
      - ./backend/.env
    depends_on:
      - ollama
    restart: unless-stopped
    runtime: nvidia  # Enable GPU access for backend (if required)
    deploy:
      resources:
        limits:
          nvidia.com/gpu.count: 2

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama  # Volume for Ollama models
      restart: unless-stopped
      runtime: nvidia  # Enable GPU access for Ollama container
    deploy:
      resources:
        limits:
          nvidia.com/gpu.count: 2

volumes:
  openwebui_data:  # Volume for OpenWebUI data (SQLite database, logs, etc.)
  ollama_data:     # Volume for Ollama models
  pptx_files:      # Volume for PowerPoint files